{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CANDOR corpus directory preprocessing for analysis\n",
        "Extract the interview files from the nested directories of the CANDOR corpus and save them in a flat directory, keeping the original names of the interview files cross the corpus.\n",
        "\n",
        "The CANDOR corpus can be downloaded from the official CANDOR download page.\n",
        "https://betterup-data-requests.herokuapp.com"
      ],
      "metadata": {
        "id": "p9Gggl4-wfxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT-rGMW6wZZa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def flatten_directory(root_dir, target_dir):\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir)\n",
        "\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.DS_Store'):\n",
        "                pass\n",
        "            # Only the 'cliffhanger' transcript is kept. Change this if you prefer another transcription output\n",
        "            elif \"audiophile\" in filename or\\\n",
        "                    \"backbiter\" in filename or\\\n",
        "                    \"transcribe\" in filename:\n",
        "                pass\n",
        "\n",
        "            else:\n",
        "                # Create a unique filename by replacing the directory separators\n",
        "                relative_path = os.path.relpath(dirpath, root_dir)\n",
        "                flattened_name = f\"{relative_path}_{filename}\".replace(os.sep, '_')\n",
        "                source_path = os.path.join(dirpath, filename)\n",
        "                target_path = os.path.join(target_dir, flattened_name)\n",
        "\n",
        "                # Copy the file to the flattened structure\n",
        "                shutil.copy2(source_path, target_path)\n",
        "                print(f\"Copied {source_path} to {target_path}\")\n",
        "\n",
        "def delete_ds_store_files(root_dir):\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename == '.DS_Store':\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                os.remove(file_path)\n",
        "                print(f\"Deleted: {file_path}\")\n",
        "\n",
        "\n",
        "### Example usage\n",
        "flatten_directory('CANDOR_Corpus/', 'CANDOR_flattened')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modify the content of the audio_video_features file\n",
        "\n",
        "you can remove the variables that you dont need for your analysis or modify variable values.\n",
        "\n",
        "Here we are removing several acoustic variables and normalizing the f0 for each speaker (user_id).\n"
      ],
      "metadata": {
        "id": "6hQFxOQaxC0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def remove_columns_from_csv(file_path, columns_to_remove):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Drop specified columns\n",
        "    df = df.drop(columns=columns_to_remove, errors='ignore')\n",
        "\n",
        "    # Save back to the same file\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Updated file saved: {file_path}\")\n",
        "\n",
        "\n",
        "def normalize_f0_per_user(file_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Ensure f0 column is numeric\n",
        "    df['f0'] = pd.to_numeric(df['f0'], errors='coerce')\n",
        "\n",
        "    # Group by user_id and normalize f0 for each user\n",
        "    df['f0_norm'] = df.groupby('user_id')['f0'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "\n",
        "    # Save the updated DataFrame back to the same file\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Data with normalized f0 column (f0_norm) saved to {file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "columns_to_remove = ['mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', \"mfcc_5\", \"mfcc_6\", \"mfcc_7\", \"mfcc_8\", \"mfcc_9\", \"mfcc_10\",\n",
        "                     \"mfcc_11\", \"mfcc_12\", \"mfcc_13\", \"mfcc_14\", \"mfcc_15\", \"mfcc_16\", \"mfcc_17\", \"mfcc_18\", \"mfcc_19\",\n",
        "                     \"mfcc_0\", \"poly_features_0\", \"poly_features_1\", \"shimmer\",\"spectral_bandwidth\",\"spectral_centroid\",\n",
        "                     \"spectral_contrast_0\",\"spectral_contrast_1\",\"spectral_contrast_2\",\"spectral_contrast_3\",\n",
        "                     \"spectral_contrast_4\",\"spectral_contrast_5\",\"spectral_contrast_6\",\"spectral_flatness\",\n",
        "                     \"spectral_rolloff\",\"zero_crossing_rate\",\"intensity\",\"jitter\",\"log_energy\",\"onset_strength\"]\n",
        "\n",
        "\n",
        "INPUT_DIRECTORY = \"CANDOR_flattened/\"\n",
        "\n",
        "\n",
        "files = sorted([file for file in os.listdir(INPUT_DIRECTORY)])\n",
        "print(len(files))\n",
        "\n",
        "for file in files:\n",
        "\n",
        "    if file.endswith(\"features.csv\"):\n",
        "        file_path = os.path.join(INPUT_DIRECTORY, file)\n",
        "\n",
        "        # remove unused columns from data\n",
        "        remove_columns_from_csv(file_path, columns_to_remove)\n",
        "\n",
        "        # scale f0 per user from 0 to 1\n",
        "        normalize_f0_per_user(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "u1ZNGAKtw3Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rename files\n",
        "\n",
        "you can use this code to rename files for simpler handling."
      ],
      "metadata": {
        "id": "aXmQW6hZyjed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "INPUT_DIRECTORY = \"CANDOR_flattened/\"\n",
        "\n",
        "files = sorted([file for file in os.listdir(INPUT_DIRECTORY)])\n",
        "print(len(files))\n",
        "\n",
        "interview_dict = {}\n",
        "interview_list = []\n",
        "interview_count = 1\n",
        "\n",
        "for file in files:\n",
        "\n",
        "    interview_name = file.split('_')[0]\n",
        "    if interview_name not in interview_dict:\n",
        "        new_interview_name = \"intrvw_\" + str(interview_count)\n",
        "        interview_dict[interview_name] = new_interview_name\n",
        "        interview_count += 1\n",
        "\n",
        "    file_path = os.path.join(INPUT_DIRECTORY, file)\n",
        "\n",
        "    new_filepath = re.sub(interview_name, interview_dict[interview_name], file_path)\n",
        "\n",
        "    print(file_path, new_filepath)\n",
        "    os.rename(file_path, new_filepath)\n",
        "\n",
        "\n",
        "print(len(interview_list))\n"
      ],
      "metadata": {
        "id": "-vQ5PVp1yk7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}